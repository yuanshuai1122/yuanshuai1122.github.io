<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>爬虫 on 🧀</title>
    <link>https://yuanshuai1122.github.io/tags/%E7%88%AC%E8%99%AB/</link>
    <description>Recent content in 爬虫 on 🧀</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-hans</language>
    <copyright>yuanshuai</copyright>
    <lastBuildDate>Fri, 26 Nov 2021 21:52:17 +0000</lastBuildDate><atom:link href="https://yuanshuai1122.github.io/tags/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Python爬虫（全）</title>
      <link>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E5%85%A8-python%E7%88%AC%E8%99%AB%E5%85%A8/</link>
      <pubDate>Fri, 26 Nov 2021 21:52:17 +0000</pubDate>
      
      <guid>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E5%85%A8-python%E7%88%AC%E8%99%AB%E5%85%A8/</guid>
      <description>（编码encode()） pat=r&amp;quot;(.*?)&amp;quot; data=re.findall(pat,reponse) print(data[0]) ```python #创建自定义opener from urllib import request #构建HTTP处理器对象（专门处理HTTP请求的对象） http_hander=request.HTTPHandler() #创建自定义open</description>
    </item>
    
    <item>
      <title>Python爬虫之数据写入</title>
      <link>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5-python%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5/</link>
      <pubDate>Fri, 26 Nov 2021 21:51:48 +0000</pubDate>
      
      <guid>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5-python%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5/</guid>
      <description>Python爬虫之数据写入 #写入到Excel import xlsxwriter #创建文件，并添加一个工作表 workbook=xlsxwriter.Workbook(&amp;#39;demo.xlsx&amp;#39;) worksheet=workbook.add_worksheet() #在指定位置写入数据 worksheet.write(&amp;#34</description>
    </item>
    
    <item>
      <title>Python爬虫之fiddler手机抓包</title>
      <link>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8Bfiddler%E6%89%8B%E6%9C%BA%E6%8A%93%E5%8C%85-python%E7%88%AC%E8%99%AB%E4%B9%8Bfiddler%E6%89%8B%E6%9C%BA%E6%8A%93%E5%8C%85md/</link>
      <pubDate>Fri, 26 Nov 2021 21:51:45 +0000</pubDate>
      
      <guid>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8Bfiddler%E6%89%8B%E6%9C%BA%E6%8A%93%E5%8C%85-python%E7%88%AC%E8%99%AB%E4%B9%8Bfiddler%E6%89%8B%E6%9C%BA%E6%8A%93%E5%8C%85md/</guid>
      <description>Python爬虫之fiddler手机抓包 fiddler官网：https://www.telerik.com/fiddler 通过Fiddler</description>
    </item>
    
    <item>
      <title>Python爬虫之scrapy框架</title>
      <link>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8Bscrapy%E6%A1%86%E6%9E%B6-python%E7%88%AC%E8%99%AB%E4%B9%8Bscrapy%E6%A1%86%E6%9E%B6/</link>
      <pubDate>Fri, 26 Nov 2021 21:51:43 +0000</pubDate>
      
      <guid>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8Bscrapy%E6%A1%86%E6%9E%B6-python%E7%88%AC%E8%99%AB%E4%B9%8Bscrapy%E6%A1%86%E6%9E%B6/</guid>
      <description>Python爬虫之scrapy框架 创建项目 scrapy startproject 项目名 创建爬虫 scrapy genspider 爬虫识别名称 &amp;lsquo;要爬取的主机地址&amp;rsquo; 运行爬虫 scrapy crawl 爬虫识别</description>
    </item>
    
    <item>
      <title>Python爬虫之验证码识别</title>
      <link>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8B%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB-python%E7%88%AC%E8%99%AB%E4%B9%8B%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB/</link>
      <pubDate>Fri, 26 Nov 2021 21:51:41 +0000</pubDate>
      
      <guid>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8B%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB-python%E7%88%AC%E8%99%AB%E4%B9%8B%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB/</guid>
      <description>Python爬虫之验证码识别 #识别车牌号 from aip import AipOcr import re APP_ID = &amp;#39;15469265&amp;#39; API_KEY = &amp;#39;rAGFtOChXtO7mnRPiwXg1Frf&amp;#39; SECRET_KEY = &amp;#39;Ailvoijh4X7lQIAoZ58UsGPlaDCmLIt7&amp;#39; client = AipOcr(APP_ID, API_KEY, SECRET_KEY) &amp;#34;&amp;#34;&amp;#34; 读取图片 &amp;#34;&amp;#34;&amp;#34; def get_file_content(filePath): with open(filePath, &amp;#39;rb&amp;#39;) as fp: return fp.read() image = get_file_content(r&amp;#39;C:\Users\Administrator\Desktop\img\ee.jpg&amp;#39;) &amp;#34;&amp;#34;&amp;#34; 调用通用文字识别, 图片</description>
    </item>
    
    <item>
      <title>Python爬虫之多线程</title>
      <link>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B-python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B/</link>
      <pubDate>Fri, 26 Nov 2021 21:51:38 +0000</pubDate>
      
      <guid>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B-python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B/</guid>
      <description>1&amp;quot;) time.sleep(1) print(&amp;ldquo;线程执行中&amp;mdash;2&amp;rdquo;) time.sleep(1) print(&amp;ldquo;线程执行中&amp;mdash;3&amp;rdquo;</description>
    </item>
    
    <item>
      <title>Python爬虫之BeautifulSoup</title>
      <link>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8Bbeautifulsoup-python%E7%88%AC%E8%99%AB%E4%B9%8Bbeautifulsoup/</link>
      <pubDate>Fri, 26 Nov 2021 21:51:35 +0000</pubDate>
      
      <guid>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8Bbeautifulsoup-python%E7%88%AC%E8%99%AB%E4%B9%8Bbeautifulsoup/</guid>
      <description>Python爬虫之BeautifulSoup #BeautifulSoup模块简介和安装 from bs4 import BeautifulSoup #CSS 选择器：BeautifulSoup4 #和lx</description>
    </item>
    
    <item>
      <title>xpath表达式</title>
      <link>https://yuanshuai1122.github.io/posts/xpath%E8%A1%A8%E8%BE%BE%E5%BC%8F-xpath%E8%A1%A8%E8%BE%BE%E5%BC%8F/</link>
      <pubDate>Fri, 26 Nov 2021 21:51:32 +0000</pubDate>
      
      <guid>https://yuanshuai1122.github.io/posts/xpath%E8%A1%A8%E8%BE%BE%E5%BC%8F-xpath%E8%A1%A8%E8%BE%BE%E5%BC%8F/</guid>
      <description>Python爬虫之xpath表达式 #xpath表达式 #有同学说，我正则用的不好，处理HTML文档很累，有没有其他的方法？ #有！那就是XPat</description>
    </item>
    
    <item>
      <title>正则表达式</title>
      <link>https://yuanshuai1122.github.io/posts/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</link>
      <pubDate>Fri, 26 Nov 2021 21:51:28 +0000</pubDate>
      
      <guid>https://yuanshuai1122.github.io/posts/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</guid>
      <description>将正则表达式转换成内部格式，提高执行效率 strr=&amp;ldquo;PYTHON666Java&amp;rdquo; pat=re.compile(r&amp;quot;Python&amp;quot;,re.I) #模式修正符：忽略大小写 print(pat.search(strr)) ```python import re #match函数和search函数 # match函数--匹配开头 #</description>
    </item>
    
    <item>
      <title>Python爬虫之urllib</title>
      <link>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8Burllib-python%E7%88%AC%E8%99%AB%E4%B9%8Burllib/</link>
      <pubDate>Fri, 26 Nov 2021 21:51:25 +0000</pubDate>
      
      <guid>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8Burllib-python%E7%88%AC%E8%99%AB%E4%B9%8Burllib/</guid>
      <description>（编码encode()） pat=r&amp;quot;(.*?)&amp;quot; data=re.findall(pat,reponse) print(data[0]) ```python #创建自定义opener from urllib import request #构建HTTP处理器对象（专门处理HTTP请求的对象） http_hander=request.HTTPHandler() #创建自定义open</description>
    </item>
    
    <item>
      <title>Python爬虫之requests</title>
      <link>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8Brequests-python%E7%88%AC%E8%99%AB%E4%B9%8Brequests/</link>
      <pubDate>Fri, 26 Nov 2021 21:51:21 +0000</pubDate>
      
      <guid>https://yuanshuai1122.github.io/posts/python%E7%88%AC%E8%99%AB%E4%B9%8Brequests-python%E7%88%AC%E8%99%AB%E4%B9%8Brequests/</guid>
      <description>Python爬虫之requests 什么是requests？ Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 通过pip install requests 可以帮你安装它。r</description>
    </item>
    
  </channel>
</rss>
