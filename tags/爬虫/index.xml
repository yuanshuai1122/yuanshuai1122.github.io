<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>爬虫 on 🧀 - 你好</title>
    <link>yuanshuai1122.github.io/tags/%E7%88%AC%E8%99%AB/</link>
    <description>Recent content in 爬虫 on 🧀 - 你好</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language><atom:link href="yuanshuai1122.github.io/tags/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Python爬虫（全）</title>
      <link>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E5%85%A8-python%E7%88%AC%E8%99%AB%E5%85%A8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E5%85%A8-python%E7%88%AC%E8%99%AB%E5%85%A8/</guid>
      <description>（编码encode()） pat=r&amp;quot;(.*?)&amp;quot; data=re.findall(pat,reponse) print(data[0]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ```python #创建自定义opener from urllib import request #构建HTTP处理器对象（</description>
    </item>
    
    <item>
      <title>Python爬虫之BeautifulSoup</title>
      <link>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8Bbeautifulsoup-python%E7%88%AC%E8%99%AB%E4%B9%8Bbeautifulsoup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8Bbeautifulsoup-python%E7%88%AC%E8%99%AB%E4%B9%8Bbeautifulsoup/</guid>
      <description>Python爬虫之BeautifulSoup 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48</description>
    </item>
    
    <item>
      <title>Python爬虫之fiddler手机抓包</title>
      <link>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8Bfiddler%E6%89%8B%E6%9C%BA%E6%8A%93%E5%8C%85-python%E7%88%AC%E8%99%AB%E4%B9%8Bfiddler%E6%89%8B%E6%9C%BA%E6%8A%93%E5%8C%85md/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8Bfiddler%E6%89%8B%E6%9C%BA%E6%8A%93%E5%8C%85-python%E7%88%AC%E8%99%AB%E4%B9%8Bfiddler%E6%89%8B%E6%9C%BA%E6%8A%93%E5%8C%85md/</guid>
      <description>Python爬虫之fiddler手机抓包 fiddler官网：https://www.telerik.com/fiddler 通过Fiddler</description>
    </item>
    
    <item>
      <title>Python爬虫之requests</title>
      <link>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8Brequests-python%E7%88%AC%E8%99%AB%E4%B9%8Brequests/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8Brequests-python%E7%88%AC%E8%99%AB%E4%B9%8Brequests/</guid>
      <description>Python爬虫之requests 什么是requests？ Requests is an elegant and simple HTTP library for Python, built for human beings. You are currently looking at the documentation of the development release. 通过pip install requests 可以帮你安装它。r</description>
    </item>
    
    <item>
      <title>Python爬虫之scrapy框架</title>
      <link>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8Bscrapy%E6%A1%86%E6%9E%B6-python%E7%88%AC%E8%99%AB%E4%B9%8Bscrapy%E6%A1%86%E6%9E%B6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8Bscrapy%E6%A1%86%E6%9E%B6-python%E7%88%AC%E8%99%AB%E4%B9%8Bscrapy%E6%A1%86%E6%9E%B6/</guid>
      <description>Python爬虫之scrapy框架 创建项目 scrapy startproject 项目名 创建爬虫 scrapy genspider 爬虫识别名称 &amp;lsquo;要爬取的主机地址&amp;rsquo; 运行爬虫 scrapy crawl 爬虫识别</description>
    </item>
    
    <item>
      <title>Python爬虫之urllib</title>
      <link>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8Burllib-python%E7%88%AC%E8%99%AB%E4%B9%8Burllib/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8Burllib-python%E7%88%AC%E8%99%AB%E4%B9%8Burllib/</guid>
      <description>（编码encode()） pat=r&amp;quot;(.*?)&amp;quot; data=re.findall(pat,reponse) print(data[0]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ```python #创建自定义opener from urllib import request #构建HTTP处理器对象（</description>
    </item>
    
    <item>
      <title>Python爬虫之多线程</title>
      <link>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B-python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B-python%E7%88%AC%E8%99%AB%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B/</guid>
      <description>1&amp;quot;) time.sleep(1) print(&amp;ldquo;线程执行中&amp;mdash;2&amp;rdquo;) time.sleep(1) print(&amp;ldquo;线程执行中&amp;mdash;3&amp;rdquo;</description>
    </item>
    
    <item>
      <title>Python爬虫之数据写入</title>
      <link>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5-python%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5-python%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5/</guid>
      <description>Python爬虫之数据写入 1 2 3 4 5 6 7 8 9 10 11 12 13 #写入到Excel import xlsxwriter #创建文件，并添加一个工作表 workbook=xlsxwriter.Workbook(&amp;#39;demo.xlsx&amp;#39;) worksheet=workbook.add_worksheet() #在指定位置写入数据 workshe</description>
    </item>
    
    <item>
      <title>Python爬虫之验证码识别</title>
      <link>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8B%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB-python%E7%88%AC%E8%99%AB%E4%B9%8B%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>yuanshuai1122.github.io/post/python%E7%88%AC%E8%99%AB%E4%B9%8B%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB-python%E7%88%AC%E8%99%AB%E4%B9%8B%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB/</guid>
      <description>Python爬虫之验证码识别 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #识别车牌号 from aip import AipOcr import re APP_ID = &amp;#39;15469265&amp;#39; API_KEY = &amp;#39;rAGFtOChXtO7mnRPiwXg1Frf&amp;#39; SECRET_KEY = &amp;#39;Ailvoijh4X7lQIAoZ58UsGPlaDCmLIt7&amp;#39; client = AipOcr(APP_ID, API_KEY, SECRET_KEY) &amp;#34;&amp;#34;&amp;#34; 读取图</description>
    </item>
    
    <item>
      <title>xpath表达式</title>
      <link>yuanshuai1122.github.io/post/xpath%E8%A1%A8%E8%BE%BE%E5%BC%8F-xpath%E8%A1%A8%E8%BE%BE%E5%BC%8F/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>yuanshuai1122.github.io/post/xpath%E8%A1%A8%E8%BE%BE%E5%BC%8F-xpath%E8%A1%A8%E8%BE%BE%E5%BC%8F/</guid>
      <description>Python爬虫之xpath表达式 1 2 3 4 5 6 7 8 9 #xpath表达式 #有同学说，我正则用的不好，处理HTML文档很累，有没有其他的方法？ #</description>
    </item>
    
    <item>
      <title>正则表达式</title>
      <link>yuanshuai1122.github.io/post/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>yuanshuai1122.github.io/post/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</guid>
      <description>将正则表达式转换成内部格式，提高执行效率 strr=&amp;ldquo;PYTHON666Java&amp;rdquo; pat=re.compile(r&amp;quot;Python&amp;quot;,re.I) #模式修正符：忽略大小写 print(pat.search(strr)) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ```python import re #match函数和search函</description>
    </item>
    
  </channel>
</rss>
